# -*- coding: utf-8 -*-
"""aitranslate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aWSgJTyvZZhB6qUiBHRHx3LUnIKJ2TvA
"""

'''
This project is an AI Audio translator that will take Spanish audio
translate it and speak it back in english with the same voice but in
the desired language(english)

Project Start Date: May 12, 2025 @9pm
Saville Atkins
'''

"""# **Install/Import All Packages**"""

!pip uninstall -y numpy
!pip install numpy==1.25.2 scipy==1.10.1 --quiet
!pip install torch torchaudio --quiet
!pip install openai-whisper --no-deps --quiet
!pip install TTS --quiet
!pip install transformers==4.36.2
!pip install deep_translator --quiet

!pip install ffmpeg
!pip install ffmpeg-python

#NOTE: RESTART RUNTIME BEFORE IMPORTING!!!!
import numpy
print("NumPy version:", numpy.__version__)

import torch
import torchaudio
print("Torch:", torch.__version__)
print("Torchaudio:", torchaudio.__version__)
import torchaudio.transforms as T
print("\n")


import whisper
print("Whisper version:", whisper.__version__)

from deep_translator import GoogleTranslator
print("Deep Translator ready")

from TTS.tts.configs.xtts_config import XttsConfig
from TTS.tts.configs.xtts_config import XttsAudioConfig
from TTS.tts.models.xtts import XttsArgs
from TTS.tts.configs.shared_configs import BaseDatasetConfig
from TTS.tts.models.xtts import Xtts
from TTS.tts.utils.text.tokenizer import TTSTokenizer
torch.serialization.add_safe_globals([XttsConfig, XttsArgs])
torch.serialization.add_safe_globals([BaseDatasetConfig])
torch.serialization.add_safe_globals([XttsAudioConfig])

from TTS.api import TTS
print("TTS ready")

"""# **Code**"""

#Mount Google Drive folder to colab
from google.colab import drive
drive.mount('/AITranslate')



from IPython.display import Audio
#import ffmpeg

#Test mount by writing to folder
with open('/AITranslate/My Drive/AITranslate/sourceaudio/greeting.txt','w') as f:
  f.write("Hello My Google Drive! I'm Saville and I'm creating a AI Translator!!! I'm so cool you know?!")
!cat /AITranslate/My\ Drive/AITranslate/sourceaudio/greeting.txt

#variable for source audio
#source_audio = '/AITranslate/My Drive/AITranslate/sourceaudio/short_arb.wav'
source_audio = '/AITranslate/My Drive/AITranslate/sourceaudio/thisone.wav'
#source_audio = '/AITranslate/My Drive/AITranslate/sourceaudio/myvoice.wav'
#source_audio = '/AITranslate/My Drive/AITranslate/sourceaudio/lilahbutbetter.wav'
#video_input = '/AITranslate/My Drive/AITranslate/sourceaudio/thisone.mp4'

'''
#get source audio from video
(
    ffmpeg.input(video_input)
    .output("video_to_wav.wav")
    .run()
)
#source_audio = 'video_to_wav.wav'
'''
Audio(source_audio)

'''
AUDIO PREPROCESSING STAGE(TorchAudio)
-Make sure audio is 16kHz
-Make sure audio is in mono
If audio doesnt satisfy those requirements
it will be converted and satisfied..AI models expect these specs(Whisper in our case)

Inputs: source_audio(filepath to thisone.wav)
Outputs:waveform(audio data),samplerate(should be 16kHz), duration(audio file length)

https://docs.pytorch.org/docs/stable/index.html
'''

#get and print metadata
metadata = torchaudio.info(source_audio)
print(metadata)

num_channels = metadata.num_channels

#load the audio file and return the waveform samplerate and number of channels
waveform, sample_rate = torchaudio.load(source_audio)
final_waveform = waveform

#check sample rate
if (sample_rate != 16000):
  resample_rate = 16000
  resampler = T.Resample(sample_rate, resample_rate, dtype=waveform.dtype)
  final_waveform = resampler(waveform)
  print("Resampled to 16kHz")


#check channels
if (num_channels > 1):
  #calculate the mean across the channel dimension
  waveform_mono = torch.mean(final_waveform, dim=0, keepdim=True)
  final_waveform = waveform_mono
  print("Converted to mono")



Audio(final_waveform.numpy().squeeze(), rate=resample_rate)

'''
Speech to Text Phase
Take the Spanish audio and generate a spanish transcript
Use Whisper OpenAi to do speech to text and translate

Input: Spanish audio file
Output: Spanish Transcript

https://github.com/openai/whisper?tab=readme-ov-file

'''

import textwrap
#Transcription
model = whisper.load_model("medium") #load turbo whisper model

result = model.transcribe(final_waveform.numpy().squeeze())
#print transcript
print("Transcript: ")
spanish_transcript = textwrap.fill(result["text"], width=80)
print(spanish_transcript)

'''
Spanish to English Text Translation Stage!

Input: Spanish Transcript
Output: English Translation

https://github.com/nidhaloff/deep-translator
'''

spanish_to_english = GoogleTranslator(source="es", target="en").translate(spanish_transcript)
#english_to_english = GoogleTranslator(source="en", target="en").translate(spanish_transcript)


english_transcript = textwrap.fill(spanish_to_english,width=80)
#english_transcript = textwrap.fill(english_to_english,width=80)
print("Transcript:", english_transcript)

'''
Speech Embedding Phase with XTTS!!

Takes the original voice audio creates a voice embedding
and then creates a replica of the original voice that will speak the english transcript

Input: English Transcript, Spanish audio file
Output: English Audio file

https://docs.coqui.ai/en/stable/models/xtts.html
'''

#load the model(xtts)
tts = TTS(model_name="tts_models/multilingual/multi-dataset/xtts_v2", gpu=torch.cuda.is_available()) #if gpu is available
#tts = TTS(model_name="tts_models/multilingual/multi-dataset/xtts_v2", gpu=False) #if gpu is not available

#Define output path
output_path = "/content/xtts_output.wav"

#Save preoprocessed waveform to a temp file
temp_speaker_wav_path = "/content/temp_speaker_wav.wav"
torchaudio.save(temp_speaker_wav_path, final_waveform, 16000)

#generate speech by cloning a voice using default settings
tts.tts_to_file(text=english_transcript,file_path=output_path,speaker_wav=temp_speaker_wav_path,language="en",split_sentences=True)

#play the output
Audio(output_path)

"""## **DONT USE OPENVOICE!!**"""

'''
Speech Embedding Phase with OpenVoice!!

Takes the original voice audio creates a voice embedding
and then creates a replica of the original voice that will speak the english transcript

Input: English Transcript, Spanish audio file
Output: English Audio file

https://github.com/myshell-ai/OpenVoice
'''

!pip uninstall -y numpy scipy

# Commented out IPython magic to ensure Python compatibility.
#Install OpenVoice
# Clone the OpenVoice repo
!git clone https://github.com/myshell-ai/OpenVoice.git
# %cd OpenVoice


# Overwrite the requirements.txt with working versions
with open("requirements.txt", "w") as f:
    f.write("numpy==1.25.2\n")
    f.write("scipy==1.10.1\n")
    f.write("librosa==0.9.1\n")
    f.write("faster-whisper==0.9.0\n")
    f.write("pydub==0.25.1\n")
    f.write("wavmark==0.0.3\n")
    f.write("eng_to_ipa==0.0.2\n")
    f.write("inflect==7.0.0\n")
    f.write("unidecode==1.3.7\n")
    f.write("whisper-timestamped==1.14.2\n")
    f.write("openai\n")
    f.write("python-dotenv\n")
    f.write("pypinyin==0.50.0\n")
    f.write("cn2an==0.5.22\n")
    f.write("jieba==0.42.1\n")
    f.write("gradio==3.48.0\n")
    f.write("langid==1.1.6\n")

# Install dependencies
!pip install -r requirements.txt

#print numpy version

!ls /content/OpenVoice

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/OpenVoice
!mkdir -p checkpoints/
!cp -r "/AITranslate/My Drive/AITranslate/converter" /content/OpenVoice/checkpoints/
!cp -r "/AITranslate/My Drive/AITranslate/base_speakers" /content/OpenVoice/checkpoints/

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/OpenVoice

import os
import torch
from openvoice import se_extractor
from openvoice.api import BaseSpeakerTTS, ToneColorConverter

#Run OpenVoice
# OpenVoice Init/setup
ckpt_base = 'checkpoints/base_speakers/EN'
ckpt_converter = 'checkpoints/converter'
device="cuda:0" if torch.cuda.is_available() else "cpu"
output_dir = 'outputs'

base_speaker_tts = BaseSpeakerTTS(f'{ckpt_base}/config.json', device=device)
base_speaker_tts.load_ckpt(f'{ckpt_base}/checkpoint.pth')

tone_color_converter = ToneColorConverter(f'{ckpt_converter}/config.json', device=device)
tone_color_converter.load_ckpt(f'{ckpt_converter}/checkpoint.pth')

os.makedirs(output_dir, exist_ok=True)

#OpenVoice provided Base speaker
source_se = torch.load(f'{ckpt_base}/en_default_se.pth').to(device)

#original audio file/Reference speaker(Voice we are cloning)
ref_speaker = '/AITranslate/My Drive/AITranslate/sourceaudio/thisone.wav'
#create speaker embedding
target_se, audio_name = se_extractor.get_se(ref_speaker, tone_color_converter, target_dir='processed', vad=True)



#----INFERENCE-----
#output location
save_path = "/content/OpenVoice/outputs/cloned_output.wav"

#source path
src_path = "/content/OpenVoice/outputs/tmp.wav"

#generate base audio from text
base_speaker_tts.tts(english_transcript,src_path, speaker='default',language='English')

#run tone color converter
tone_color_converter.convert(audio_src_path=src_path, src_se=source_se, tgt_se=target_se, output_path=save_path)

print("target_se shape:", target_se.shape)
print("target_se sample values:", target_se[:10])
Audio(save_path)

Audio(src_path)